{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhtuan/anaconda3/envs/test-phobert/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (BertConfig, AdamW,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "from transformers import (BertTokenizer, RobertaTokenizer, XLMRobertaTokenizer,\n",
    "                           AlbertTokenizer)\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from src.model import OneIE\n",
    "from src.graph import Graph\n",
    "from src.config import Config\n",
    "from src.data import IEDataset\n",
    "from src.scorer import score_graphs\n",
    "from src.util import generate_vocabs, load_valid_patterns, save_result, best_score_by_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file: /home/mhtuan/work/ee/event-extraction-oneie/output/20250122_104016/log.txt\n"
     ]
    }
   ],
   "source": [
    "config = Config.from_json_file(path=\"/home/mhtuan/work/ee/event-extraction-oneie/config/config.json\")\n",
    "use_gpu = config.use_gpu\n",
    "if use_gpu and config.gpu_device >= 0:\n",
    "    torch.cuda.set_device(config.gpu_device)\n",
    "\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "output_dir = os.path.join(config.log_path, timestamp)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "log_file = os.path.join(output_dir, 'log.txt')\n",
    "with open(log_file, 'w', encoding='utf-8') as w:\n",
    "    w.write(json.dumps(config.to_dict()) + '\\n')\n",
    "    print('Log file: {}'.format(log_file))\n",
    "    \n",
    "best_role_model = os.path.join(output_dir, 'best.role.mdl')\n",
    "dev_result_file = os.path.join(output_dir, 'result.dev.json')\n",
    "test_result_file = os.path.join(output_dir, 'result.test.json')\n",
    "last_model = os.path.join(output_dir, 'last.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config.bert_model_name\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name,\n",
    "#                                           cache_dir=config.bert_cache_dir,\n",
    "#                                           do_lower_case=False)\n",
    "if config.bert_model_name.startswith('bert-'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name, # cache_dir=config.bert_cache_dir,\n",
    "                                              do_lower_case=False)\n",
    "elif config.bert_model_name.startswith('roberta-'):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name, # cache_dir=config.bert_cache_dir,\n",
    "                                            do_lower_case=False)\n",
    "elif config.bert_model_name.startswith('xlm-roberta-'):\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(model_name, # cache_dir=config.bert_cache_dir,\n",
    "                                            do_lower_case=False)\n",
    "elif config.bert_model_name.startswith('albert-'):\n",
    "    # \"albert-xlarge-v2\"\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_name, # cache_dir=config.bert_cache_dir,\n",
    "                                                do_lower_case=False)\n",
    "else:\n",
    "    raise ValueError('Unknown model: {}'.format(config.bert_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded 4688 overlength instances\n",
      "Loaded 2358 instances from /home/mhtuan/work/ee/event-extraction-oneie/data/rams/processed-data/oneie/train_oneie.json\n",
      "Discarded 600 overlength instances\n",
      "Loaded 309 instances from /home/mhtuan/work/ee/event-extraction-oneie/data/rams/processed-data/oneie/dev_oneie.json\n",
      "Discarded 575 overlength instances\n",
      "Loaded 276 instances from /home/mhtuan/work/ee/event-extraction-oneie/data/rams/processed-data/oneie/test_oneie.json\n"
     ]
    }
   ],
   "source": [
    "train_set = IEDataset(config.train_file, gpu=use_gpu,\n",
    "                      relation_mask_self=config.relation_mask_self,\n",
    "                      relation_directional=config.relation_directional,\n",
    "                      symmetric_relations=config.symmetric_relations,\n",
    "                      ignore_title=config.ignore_title,\n",
    "                      max_length=config.sent_max_length)\n",
    "dev_set = IEDataset(config.dev_file, gpu=use_gpu,\n",
    "                    relation_mask_self=config.relation_mask_self,\n",
    "                    relation_directional=config.relation_directional,\n",
    "                    symmetric_relations=config.symmetric_relations,\n",
    "                     max_length=config.sent_max_length)\n",
    "test_set = IEDataset(config.test_file, gpu=use_gpu,\n",
    "                     relation_mask_self=config.relation_mask_self,\n",
    "                     relation_directional=config.relation_directional,\n",
    "                     symmetric_relations=config.symmetric_relations,\n",
    "                     max_length=config.sent_max_length)\n",
    "vocabs = generate_vocabs([train_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.numberize(tokenizer, vocabs)\n",
    "dev_set.numberize(tokenizer, vocabs)\n",
    "test_set.numberize(tokenizer, vocabs)\n",
    "valid_patterns = load_valid_patterns(config.valid_pattern_path, vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained BERT model bert-large-cased\n"
     ]
    }
   ],
   "source": [
    "batch_num = len(train_set) // config.batch_size\n",
    "dev_batch_num = len(dev_set) // config.eval_batch_size + \\\n",
    "    (len(dev_set) % config.eval_batch_size != 0)\n",
    "test_batch_num = len(test_set) // config.eval_batch_size + \\\n",
    "    (len(test_set) % config.eval_batch_size != 0)\n",
    "\n",
    "# initialize the model\n",
    "model = OneIE(config, vocabs, valid_patterns)\n",
    "model.load_bert(model_name, cache_dir=config.bert_cache_dir)\n",
    "if use_gpu:\n",
    "    model.cuda(device=config.gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_name = config.bert_model_name.split('-')[0]\n",
    "param_groups = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if n.startswith(transformer_name)],\n",
    "        'lr': config.bert_learning_rate, 'weight_decay': config.bert_weight_decay\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if not n.startswith(transformer_name)\n",
    "                   and 'crf' not in n and 'global_feature' not in n],\n",
    "        'lr': config.learning_rate, 'weight_decay': config.weight_decay\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if not n.startswith(transformer_name)\n",
    "                   and ('crf' in n or 'global_feature' in n)],\n",
    "        'lr': config.learning_rate, 'weight_decay': 0\n",
    "    }\n",
    "]\n",
    "optimizer = AdamW(params=param_groups)\n",
    "schedule = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps=batch_num * config.warmup_epoch,\n",
    "                                           num_training_steps=batch_num * config.max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "state = dict(model=model.state_dict(),\n",
    "             config=config.to_dict(),\n",
    "             vocabs=vocabs,\n",
    "             valid=valid_patterns)\n",
    "\n",
    "global_step = 0\n",
    "global_feature_max_step = int(config.global_warmup * batch_num) + 1\n",
    "print('global feature max step:', global_feature_max_step)\n",
    "\n",
    "tasks = ['entity', 'trigger', 'relation', 'role']\n",
    "best_dev = {k: 0 for k in tasks}\n",
    "for epoch in range(config.max_epoch):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "\n",
    "    # training set\n",
    "    progress = tqdm.tqdm(total=batch_num, ncols=75,\n",
    "                         desc='Train {}'.format(epoch))\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, batch in enumerate(DataLoader(\n",
    "            train_set, batch_size=config.batch_size // config.accumulate_step,\n",
    "            shuffle=True, drop_last=True, collate_fn=train_set.collate_fn)):\n",
    "\n",
    "        loss = model(batch)\n",
    "        loss = loss * (1 / config.accumulate_step)\n",
    "        loss.backward()\n",
    "\n",
    "        if (batch_idx + 1) % config.accumulate_step == 0:\n",
    "            progress.update(1)\n",
    "            global_step += 1\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), config.grad_clipping)\n",
    "            optimizer.step()\n",
    "            schedule.step()\n",
    "            optimizer.zero_grad()\n",
    "    progress.close()\n",
    "\n",
    "    # dev set\n",
    "    progress = tqdm.tqdm(total=dev_batch_num, ncols=75,\n",
    "                         desc='Dev {}'.format(epoch))\n",
    "    best_dev_role_model = False\n",
    "    dev_gold_graphs, dev_pred_graphs, dev_sent_ids, dev_tokens = [], [], [], []\n",
    "    for batch in DataLoader(dev_set, batch_size=config.eval_batch_size,\n",
    "                            shuffle=False, collate_fn=dev_set.collate_fn):\n",
    "        progress.update(1)\n",
    "        graphs = model.predict(batch)\n",
    "        if config.ignore_first_header:\n",
    "            for inst_idx, sent_id in enumerate(batch.sent_ids):\n",
    "                if int(sent_id.split('-')[-1]) < 4:\n",
    "                    graphs[inst_idx] = Graph.empty_graph(vocabs)\n",
    "        for graph in graphs:\n",
    "            graph.clean(relation_directional=config.relation_directional,\n",
    "                        symmetric_relations=config.symmetric_relations)\n",
    "        dev_gold_graphs.extend(batch.graphs)\n",
    "        dev_pred_graphs.extend(graphs)\n",
    "        dev_sent_ids.extend(batch.sent_ids)\n",
    "        dev_tokens.extend(batch.tokens)\n",
    "    progress.close()\n",
    "    dev_scores = score_graphs(dev_gold_graphs, dev_pred_graphs,\n",
    "                              relation_directional=config.relation_directional)\n",
    "    for task in tasks:\n",
    "        writer.add_scalar(f'data/dev/{task}', dev_scores[task]['f'], epoch) \n",
    "        if dev_scores[task]['f'] > best_dev[task]:\n",
    "            best_dev[task] = dev_scores[task]['f']\n",
    "            if task == 'role':\n",
    "                print('Saving best role model')\n",
    "                torch.save(state, best_role_model)\n",
    "                best_dev_role_model = True\n",
    "                save_result(dev_result_file,\n",
    "                            dev_gold_graphs, dev_pred_graphs, dev_sent_ids,\n",
    "                            dev_tokens)\n",
    "\n",
    "    # test set\n",
    "    progress = tqdm.tqdm(total=test_batch_num, ncols=75,\n",
    "                         desc='Test {}'.format(epoch))\n",
    "    test_gold_graphs, test_pred_graphs, test_sent_ids, test_tokens = [], [], [], []\n",
    "    for batch in DataLoader(test_set, batch_size=config.eval_batch_size, shuffle=False,\n",
    "                            collate_fn=test_set.collate_fn):\n",
    "        progress.update(1)\n",
    "        graphs = model.predict(batch)\n",
    "        if config.ignore_first_header:\n",
    "            for inst_idx, sent_id in enumerate(batch.sent_ids):\n",
    "                if int(sent_id.split('-')[-1]) < 4:\n",
    "                    graphs[inst_idx] = Graph.empty_graph(vocabs)\n",
    "        for graph in graphs:\n",
    "            graph.clean(relation_directional=config.relation_directional,\n",
    "                        symmetric_relations=config.symmetric_relations)\n",
    "        test_gold_graphs.extend(batch.graphs)\n",
    "        test_pred_graphs.extend(graphs)\n",
    "        test_sent_ids.extend(batch.sent_ids)\n",
    "        test_tokens.extend(batch.tokens)\n",
    "    progress.close()\n",
    "    test_scores = score_graphs(test_gold_graphs, test_pred_graphs,\n",
    "                               relation_directional=config.relation_directional)\n",
    "    for task in tasks:\n",
    "        writer.add_scalar(f'data/test/{task}', test_scores[task]['f'], epoch) \n",
    "    if best_dev_role_model:\n",
    "        save_result(test_result_file, test_gold_graphs, test_pred_graphs,\n",
    "                    test_sent_ids, test_tokens)\n",
    "\n",
    "    result = json.dumps(\n",
    "        {'epoch': epoch, 'dev': dev_scores, 'test': test_scores})\n",
    "    with open(log_file, 'a', encoding='utf-8') as w:\n",
    "        w.write(result + '\\n')\n",
    "    print('Log file', log_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-phobert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
