{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONEIE Demo page\n",
    "This notebook demo's ONEIE model for event extraction. The notebook uses the author's trained model. The paper can be found [here](https://www.aclweb.org/anthology/2020.acl-main.713/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import tqdm\n",
    "import traceback\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "cur_dir = Path.cwd()\n",
    "sys.path.append(str(cur_dir.parents[0] / 'oneie'))\n",
    "\n",
    "from model import OneIE\n",
    "from config import Config\n",
    "from util import save_result\n",
    "from data import IEDatasetEval, InstanceLdcEval, BatchLdcEval\n",
    "from convert import json_to_cs\n",
    "\n",
    "\n",
    "\n",
    "format_ext_mapping = {'txt': 'txt', 'ltf': 'ltf.xml', 'json': 'json',\n",
    "                      'json_single': 'json'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the Model\n",
    "The model can be downloaded [in this link](http://blender.cs.illinois.edu/software/oneie/).  \n",
    "Be sure to save the model in `oneie/models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model from /home/vinitrinh/Desktop/Event Extraction/ONEIE/predictions/eai-dsta/oneie/models/best.role.mdl\n"
     ]
    }
   ],
   "source": [
    "from predict import load_model\n",
    "model_path = cur_dir.parents[0] / 'oneie' / 'models' / 'best.role.mdl'\n",
    "model, tokenizer, config = load_model(model_path, \n",
    "                                      device=0, \n",
    "                                      gpu=False,\n",
    "                                      beam_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess\n",
    "Some preprocessing is needed before the model can predict them. First we take the raw text and separate it into word tokens. The expected output show below as:  \n",
    "`[(doc_id_str), [(token1, 0,1), (token2,1,2) ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asd0',\n",
       "  [('Prime', 0, 1),\n",
       "   ('Minister', 1, 2),\n",
       "   ('Abdullah', 2, 3),\n",
       "   ('Gul', 3, 4),\n",
       "   ('resigned', 4, 5),\n",
       "   ('earlier', 5, 6),\n",
       "   ('Tuesday', 6, 7),\n",
       "   ('to', 7, 8),\n",
       "   ('make', 8, 9),\n",
       "   ('way', 9, 10),\n",
       "   ('for', 10, 11),\n",
       "   ('Erdogan', 11, 12),\n",
       "   (',', 12, 13),\n",
       "   ('who', 13, 14),\n",
       "   ('won', 14, 15),\n",
       "   ('a', 15, 16),\n",
       "   ('parliamentary', 16, 17),\n",
       "   ('seat', 17, 18),\n",
       "   ('in', 18, 19),\n",
       "   ('by-elections', 19, 20),\n",
       "   ('Sunday', 20, 21),\n",
       "   ('.', 21, 22)])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Prime Minister Abdullah Gul resigned earlier Tuesday to make way for Erdogan, who won a parliamentary seat in by-elections Sunday.\"\n",
    "def text_to_tokens(text):\n",
    "    \"\"\"\n",
    "    this tokenizes the text into words according to NLTK's model\n",
    "    The important output is doc, which contains:\n",
    "        (1) doc id\n",
    "        (2) tokens\n",
    "    \"\"\"\n",
    "    doc_id = 'asd'\n",
    "    offset = 0\n",
    "    doc_tokens = []\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [(token, offset + i, offset + i + 1)\n",
    "              for i, token in enumerate(tokens)]\n",
    "    doc_tokens.append(('asd0', tokens))\n",
    "    return doc_tokens, tokens\n",
    "\n",
    "doc_tokens, tokens = text_to_tokens(text)\n",
    "doc_tokens[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the important preprocessing happens in `numberize`, which is a method to the `IEDatasetEval` in `data.py`.   \n",
    "</br>\n",
    "\n",
    "As the name suggests, it changes the string tokens into their integer indices, eg, 'asd' token corresponds to index 345 in the word embedding. The resulting output has all of BERT's attention piece idx and attention masks etc.  \n",
    "</br>\n",
    "\n",
    "One misleading name is the `token_idx` which is the token index which has nothing to do with the models tokenizer. It tracks the specific word in the document and follows the format `<doc-id>-<word-id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InstanceLdcEval(sent_id='asd0', tokens=['Prime', 'Minister', 'Abdullah', 'Gul', 'resigned', 'earlier', 'Tuesday', 'to', 'make', 'way', 'for', 'Erdogan', ',', 'who', 'won', 'a', 'parliamentary', 'seat', 'in', 'by-elections', 'Sunday', '.'], token_ids=['asd:0-1', 'asd:1-2', 'asd:2-3', 'asd:3-4', 'asd:4-5', 'asd:5-6', 'asd:6-7', 'asd:7-8', 'asd:8-9', 'asd:9-10', 'asd:10-11', 'asd:11-12', 'asd:12-13', 'asd:13-14', 'asd:14-15', 'asd:15-16', 'asd:16-17', 'asd:17-18', 'asd:18-19', 'asd:19-20', 'asd:20-21', 'asd:21-22'], pieces=['Prime', 'Minister', 'Abdullah', 'G', '##ul', 'resigned', 'earlier', 'Tuesday', 'to', 'make', 'way', 'for', 'E', '##rdo', '##gan', ',', 'who', 'won', 'a', 'parliamentary', 'seat', 'in', 'by', '-', 'elections', 'Sunday', '.'], piece_idxs=[101, 3460, 2110, 14677, 144, 4654, 4603, 2206, 9667, 1106, 1294, 1236, 1111, 142, 16525, 3820, 117, 1150, 1281, 170, 6774, 1946, 1107, 1118, 118, 3212, 3625, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_lens=[1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def numberize(data, tokenizer):\n",
    "    numberized_data = []\n",
    "    for i, (sent_id, sent_tokens) in enumerate(data):\n",
    "        tokens = []\n",
    "        token_ids = []\n",
    "        pieces = []\n",
    "        token_lens = []\n",
    "        for token_text, start_char, end_char in sent_tokens:\n",
    "            token_id = '{}:{}-{}'.format(\"asd\", start_char, end_char)\n",
    "            token_pieces = [p for p in tokenizer.tokenize(token_text) if p]\n",
    "            if len(token_pieces) == 0:\n",
    "                continue\n",
    "            tokens.append(token_text)\n",
    "            pieces.extend(token_pieces)\n",
    "            token_lens.append(len(token_pieces))\n",
    "            token_ids.append(token_id)\n",
    "\n",
    "        # skip overlength sentences, set max_length = 200 for purpose of demo\n",
    "        if len(pieces) > 200 - 2:\n",
    "            continue\n",
    "        # skip empty sentences\n",
    "        if len(pieces) == 0:\n",
    "            continue\n",
    "\n",
    "        # pad word pieces with special tokens\n",
    "        piece_idxs = tokenizer.encode(pieces,\n",
    "                                      add_special_tokens=True,\n",
    "                                      max_length=200)\n",
    "        pad_num = 200 - len(piece_idxs)\n",
    "        attn_mask = [1] * len(piece_idxs) + [0] * pad_num\n",
    "        piece_idxs = piece_idxs + [0] * pad_num\n",
    "\n",
    "        instance = InstanceLdcEval(\n",
    "            sent_id=sent_id,\n",
    "            tokens=tokens,\n",
    "            token_ids=token_ids,\n",
    "            pieces=pieces,\n",
    "            piece_idxs=piece_idxs,\n",
    "            token_lens=token_lens,\n",
    "            attention_mask=attn_mask\n",
    "        )\n",
    "        numberized_data.append(instance)\n",
    "    return numberized_data\n",
    "\n",
    "numberize(doc_tokens, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oneie's model class predicts instances as batches (only), so it doesn't accept the `InstanceLdcEval` we just created. So we copy and paste a `collate_fn` here, which is similarly from `IEDatasetEval` to make a batch of `InstanceLdcEval` into `BatchLdcEval` for this demo's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_piece_idxs = []\n",
    "    batch_tokens = []\n",
    "    batch_token_lens = []\n",
    "    batch_attention_masks = []\n",
    "    batch_sent_ids = []\n",
    "    batch_token_ids = []\n",
    "    batch_token_nums = []\n",
    "\n",
    "    for inst in batch:\n",
    "        token_num = len(inst.tokens)\n",
    "        batch_piece_idxs.append(inst.piece_idxs)\n",
    "        batch_attention_masks.append(inst.attention_mask)\n",
    "        batch_token_lens.append(inst.token_lens)\n",
    "        batch_tokens.append(inst.tokens)\n",
    "        batch_sent_ids.append(inst.sent_id)\n",
    "        batch_token_ids.append(inst.token_ids)\n",
    "        batch_token_nums.append(len(inst.tokens))\n",
    "\n",
    "    batch_piece_idxs = torch.LongTensor(batch_piece_idxs)\n",
    "    batch_attention_masks = torch.FloatTensor(\n",
    "        batch_attention_masks)\n",
    "    batch_token_nums = torch.LongTensor(batch_token_nums)\n",
    "\n",
    "    return BatchLdcEval(sent_ids=batch_sent_ids,\n",
    "                        token_ids=batch_token_ids,\n",
    "                        tokens=batch_tokens,\n",
    "                        piece_idxs=batch_piece_idxs,\n",
    "                        token_lens=batch_token_lens,\n",
    "                        attention_masks=batch_attention_masks,\n",
    "                        token_nums=batch_token_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the steps are collated here. These functions (not oneie's source code) are written to help us eyeball the output of oneie's events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    data, tokens = text_to_tokens(text)\n",
    "    data = numberize(data, tokenizer)\n",
    "    data = collate_fn(data)\n",
    "    return data, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_trigger_roles(graph, text, tokens):\n",
    "    print(text+'\\n')\n",
    "    \n",
    "    print(\"entities:\")\n",
    "    collect = False\n",
    "    entity_strings = []\n",
    "    for entity in graph.entities:\n",
    "        start, end, entity_type = entity\n",
    "\n",
    "        matched_tokens = []        \n",
    "        for token in tokens:\n",
    "            if (token[1]==start):\n",
    "                collect = True\n",
    "            if collect == True: matched_tokens.append(token[0])\n",
    "            if end in [token[1], token[2]]:\n",
    "                collect = False\n",
    "\n",
    "        entity_itos = {i: s for s, i in graph.vocabs['entity_type'].items()}\n",
    "        entity_label = entity_itos[entity_type]\n",
    "        entity_strings.append(matched_tokens)\n",
    "        print(f\"   {' '.join(matched_tokens)} - {entity_label}\")\n",
    "\n",
    "\n",
    "    print(\"\\ntriggers:\")\n",
    "    collect = False\n",
    "    trigger_class_strings = []\n",
    "    for trigger in graph.triggers:\n",
    "        start, end, trigger_type = trigger\n",
    "        \n",
    "        matched_tokens = []\n",
    "        for token in tokens:\n",
    "            if (token[1]==start):\n",
    "                collect = True\n",
    "            if collect == True: matched_tokens.append(token[0])\n",
    "            if end in [token[1], token[2]]:\n",
    "                collect = False\n",
    "\n",
    "        trigger_itos = {i: s for s, i in graph.vocabs['event_type'].items()}\n",
    "        trigger_label = trigger_itos[trigger_type]\n",
    "        trigger_class_strings.append(trigger_label)\n",
    "        print(f\"   {' '.join(matched_tokens)} - {trigger_label}\")\n",
    "\n",
    "    print(\"\\nroles:\")\n",
    "    for role in graph.roles:\n",
    "        trigger_idx, entity_idx, role_type = role\n",
    "        \n",
    "        trigger_class_string = trigger_class_strings[trigger_idx]\n",
    "        entity_string = entity_strings[entity_idx]\n",
    "        role_itos = {i: s for s, i in graph.vocabs['role_type'].items()}\n",
    "        role_label = role_itos[role_type]\n",
    "        \n",
    "        print(f\"   {' '.join(entity_string)} - {role_label} - {trigger_class_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(text):\n",
    "    data, tokens = prepare_text(text)\n",
    "    graph = model.predict(data)\n",
    "\n",
    "    graph = graph[0]\n",
    "    graph.clean(relation_directional=config.relation_directional,\n",
    "                symmetric_relations=config.symmetric_relations)\n",
    "\n",
    "    examine_trigger_roles(graph, text, tokens)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Demo model\n",
    "Input text here for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime Minister Abdullah Gul resigned earlier Tuesday to make way for Erdogan, who won a parliamentary seat in by-elections Sunday.\n",
      "\n",
      "entities:\n",
      "   Minister - PER\n",
      "   Abdullah Gul - PER\n",
      "   Erdogan - PER\n",
      "   who - PER\n",
      "   parliamentary - ORG\n",
      "\n",
      "triggers:\n",
      "   resigned - Personnel:End-Position\n",
      "   won - Personnel:Elect\n",
      "\n",
      "roles:\n",
      "   Prime - Person\n",
      "   Minister - Person\n"
     ]
    }
   ],
   "source": [
    "text = \"Prime Minister Abdullah Gul resigned earlier Tuesday to make way for Erdogan, who won a parliamentary seat in by-elections Sunday.\"\n",
    "graph = demo(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A civilian aid worker from San Francisco was killed in an attack in Afghanistan\n",
      "\n",
      "entities:\n",
      "   civilian - PER\n",
      "   worker - PER\n",
      "   San Francisco - GPE\n",
      "   Afghanistan - GPE\n",
      "\n",
      "triggers:\n",
      "   killed - Life:Die\n",
      "   attack - Conflict:Attack\n",
      "\n",
      "roles:\n",
      "   A - Victim\n",
      "   civilian - Target\n",
      "   A civilian aid - Place\n",
      "   civilian aid - Place\n"
     ]
    }
   ],
   "source": [
    "text = \"A civilian aid worker from San Francisco was killed in an attack in Afghanistan\"\n",
    "graph = demo(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_CF_20030303.1900.02 In San Francisco there is Harry no event in this sentence\n",
      "\n",
      "entities:\n",
      "   San Francisco - GPE\n",
      "   Harry - PER\n",
      "\n",
      "triggers:\n",
      "\n",
      "roles:\n"
     ]
    }
   ],
   "source": [
    "text = \"CNN_CF_20030303.1900.02 HAIn San Francisco there is Harry no event in this sentence\"\n",
    "graph = demo(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Oneie's key output: graph\n",
    "One key output in this process is the `graph` object that oneie outputs. All the entity mentions and triggers etc, are directly available from the class but the `to_dict` method nicely brings it all out conveniently for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'add_entity',\n",
       " 'add_relation',\n",
       " 'add_role',\n",
       " 'add_trigger',\n",
       " 'clean',\n",
       " 'copy',\n",
       " 'empty_graph',\n",
       " 'entities',\n",
       " 'entity_num',\n",
       " 'entity_scores',\n",
       " 'graph_local_score',\n",
       " 'mentions',\n",
       " 'relation_num',\n",
       " 'relation_scores',\n",
       " 'relations',\n",
       " 'role_num',\n",
       " 'role_scores',\n",
       " 'roles',\n",
       " 'to_dict',\n",
       " 'to_label_idxs',\n",
       " 'trigger_num',\n",
       " 'trigger_scores',\n",
       " 'triggers',\n",
       " 'vocabs']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [], 'triggers': [], 'relations': [], 'roles': []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
